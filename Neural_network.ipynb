{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ReLU Activation"
      ],
      "metadata": {
        "id": "78vwmDZ7geSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "np.random.seed(0)\n",
        "\n",
        "X = [[1,2,3,2.5],\n",
        "     [2.0,5.0,-1.0,2.0],\n",
        "     [-1.5,2.7,3.3,-0.8]]\n",
        "\n",
        "\n",
        "X,y = spiral_data(100, 3)\n",
        "\n",
        "class Layer_Dense:\n",
        "  def __init__(self,n_inputs,n_neurons):\n",
        "    self.weights = 0.10 * np.random.randn(n_inputs,n_neurons)\n",
        "    self.biases =  np.zeros((1,n_neurons))\n",
        "  def forward(self,inputs):\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "\n",
        "class Activation_ReLU:\n",
        "  def forward(self,inputs):\n",
        "    self.output = np.maximum(0, inputs)\n",
        "\n",
        "\n",
        "layer1 = Layer_Dense(2,5)\n",
        "activation1 = Activation_ReLU()\n",
        "#layer2 = Layer_Dense(5,2)      # layer 2 take layer1 output as a input\n",
        "\n",
        "\n",
        "#print(layer1.output)\n",
        "layer1.forward(X)\n",
        "activation1.forward(layer1.output)\n",
        "#print(layer1.output)                   # this may contain the negative values\n",
        "print(activation1.output)               # this will make the negative values to 0 and positive values as it is.\n",
        "#layer2.forward(layer1.output)\n",
        "#print(layer2.output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trl1qhWEPfeT",
        "outputId": "0759302b-24ef-4199-8c07-51cf7a7709cc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 4.65504526e-04\n",
            "  4.56845892e-05]\n",
            " [0.00000000e+00 5.93467943e-05 0.00000000e+00 2.03573189e-04\n",
            "  6.10024276e-04]\n",
            " ...\n",
            " [1.13291515e-01 0.00000000e+00 0.00000000e+00 8.11079627e-02\n",
            "  0.00000000e+00]\n",
            " [1.34588354e-01 0.00000000e+00 3.09493973e-02 5.66337522e-02\n",
            "  0.00000000e+00]\n",
            " [1.07817915e-01 0.00000000e+00 0.00000000e+00 8.72561871e-02\n",
            "  0.00000000e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7N_VVFBRPfk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exponential"
      ],
      "metadata": {
        "id": "BtBn72rspHTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Exponential\n",
        "\n",
        "import math\n",
        "\n",
        "layer_outputs = [4.8, 1.12, 2.385]\n",
        "\n",
        "#E = 2.71828182846\n",
        "E = math.e\n",
        "\n",
        "exp_values = []\n",
        "\n",
        "for output in layer_outputs:\n",
        "  exp_values.append(E**output)\n",
        "\n",
        "print(exp_values)\n",
        "\n",
        "\n",
        "norm_base = sum(exp_values)\n",
        "norm_values = []\n",
        "\n",
        "for value in exp_values:\n",
        "  norm_values.append(value / norm_base)\n",
        "\n",
        "print(norm_values)\n",
        "print(sum(norm_values))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-TXP01PPfnm",
        "outputId": "0ed6c36f-a05f-40e5-f302-d3de565055fe"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[121.51041751873483, 3.0648542032930024, 10.859062664920513]\n",
            "[0.8971906427477127, 0.0226298170044269, 0.08017954024786043]\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "\n",
        "layer_outputs = [4.8, 1.12, 2.385]\n",
        "\n",
        "exp_values = np.exp(layer_outputs)\n",
        "\n",
        "norm_values = exp_values / np.sum(exp_values)\n",
        "\n",
        "print(norm_values)\n",
        "print(sum(norm_values))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIraNHiCXzDy",
        "outputId": "57a8c760-401e-46c6-b08a-6b74e4ee8ff9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.89719064 0.02262982 0.08017954]\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CpZ9MkgjZ00c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import nnfs\n",
        "\n",
        "layer_outputs = [[4.8,1.21,2.385],\n",
        "                 [8.9,-1.81,0.2],\n",
        "                 [1.41,1.051,0.026]]\n",
        "\n",
        "\n",
        "exp_values = np.exp(layer_outputs)\n",
        "\n",
        "#print(np.sum(layer_outputs,axis=1,keepdims=True))     # axis=1 is for rows, axis=0 is for column and by default it is None\n",
        "\n",
        "norm_values = exp_values / np.sum(exp_values,axis=1,keepdims=True)\n",
        "\n",
        "print(norm_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_pvWYCoaLWe",
        "outputId": "80b6743b-59e3-49a3-9adf-a8037d4c9397"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[8.95282664e-01 2.47083068e-02 8.00090293e-02]\n",
            " [9.99811129e-01 2.23163963e-05 1.66554348e-04]\n",
            " [5.13097164e-01 3.58333899e-01 1.28568936e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Softmax Activation"
      ],
      "metadata": {
        "id": "5DIneasdgS0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "np.random.seed(0)\n",
        "\n",
        "X,y = spiral_data(100, 3)\n",
        "\n",
        "class Layer_Dense:\n",
        "  def __init__(self,n_inputs,n_neurons):\n",
        "    self.weights = 0.10 * np.random.randn(n_inputs,n_neurons)\n",
        "    self.biases =  np.zeros((1,n_neurons))\n",
        "  def forward(self,inputs):\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "\n",
        "class Activation_ReLU:\n",
        "  def forward(self,inputs):\n",
        "    self.output = np.maximum(0, inputs)\n",
        "\n",
        "class Activation_Softmax:\n",
        "  def forward(self, inputs):\n",
        "    exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))             # it will normalise the values as the exponential make the value to be in between 0 and 1.\n",
        "    probablities = exp_values / np.sum(exp_values,axis=1, keepdims=True)\n",
        "    self.output = probablities\n",
        "\n",
        "X,y =spiral_data(samples =100, classes=3)\n",
        "\n",
        "dense1 = Layer_Dense(2,3)\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "dense2 = Layer_Dense(3,3)\n",
        "activation2 = Activation_Softmax()\n",
        "\n",
        "dense1.forward(X)\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "dense2.forward(activation1.output)\n",
        "activation2.forward(dense2.output)\n",
        "print(activation2.output[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIGRqWfna1A-",
        "outputId": "c5b2843a-4d7d-43b7-e12f-27654593853b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.33333333 0.33333333 0.33333333]\n",
            " [0.3333351  0.33333329 0.33333161]\n",
            " [0.33327101 0.33333011 0.33339888]\n",
            " [0.33329674 0.33330456 0.3333987 ]\n",
            " [0.33331198 0.33331375 0.33337426]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "74o1CmIJewH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss with categorical cross entropy"
      ],
      "metadata": {
        "id": "xMoYiMRIhIRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "softmax_output = [0.7,0.1,0.2]\n",
        "target_output = [1,0,0]\n",
        "\n",
        "\n",
        "loss = -(math.log(softmax_output[0])* target_output[0] +\n",
        "         math.log(softmax_output[1])* target_output[1]+\n",
        "         math.log(softmax_output[2])* target_output[2])\n",
        "\n",
        "print(loss)\n",
        "\n",
        "print(-math.log(0.7))\n",
        "print(-math.log(0.5))                              # as the value in the log increases the overall value decreases. confidence belongs to [0,1] and loss belongs to (-inf,0]."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2duKnaXhQ7v",
        "outputId": "b7c52f50-022b-490b-b5bc-19f7202d90f8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.35667494393873245\n",
            "0.35667494393873245\n",
            "0.6931471805599453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pyAbanU8hchP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "softmax_outputs = np.array([[0.7,0.1,0.2],\n",
        "                            [0.1,0.5,0.4],\n",
        "                            [0.02,0.9,0.08]])\n",
        "\n",
        "class_targets = [0,1,1]\n",
        "\n",
        "print(softmax_outputs[[0,-1,-2], class_targets])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JBZQt2ujk3q",
        "outputId": "1a257efe-a02b-4c19-fe11-cf1a07810ab6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.7 0.9 0.5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss:\n",
        "  def calculate(self,output,y):\n",
        "    sample_losses = self.forward(output,y)\n",
        "    data_loss = np.mean(sample_losses)\n",
        "    return data_loss\n",
        "\n",
        "class Loss_CategoricalCrossEntropy(Loss):\n",
        "  def forward(self,y_pred, y_true):\n",
        "    samples = len(y_pred)\n",
        "    y_pred_clipped = np.clip(y_pred, 1e-7 , 1-1e-7)\n",
        "\n",
        "\n",
        "    if len(y_true.shape) == 1:\n",
        "      correct_confidences = y_pred_clipped[range(samples), y_true]\n",
        "    elif len(y_true.shape) == 2:\n",
        "      correct_confidences = np.sum(y_pred_clipped*y_true, axis=1)\n",
        "\n",
        "    negative_log_likelihoods = -np.log(correct_confidences)\n",
        "    return negative_log_likelihoods\n",
        "\n",
        "\n",
        "loss_function = Loss_CategoricalCrossEntropy()\n",
        "loss = loss_function.calculate(activation2.output,y)\n",
        "\n",
        "print(\"Loss :\" , loss)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rV4pXQeYkGV3",
        "outputId": "dd703dbc-6e07-4108-a8ef-29aca162aa2d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss : 1.0985500224813294\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "439AYh5TnlXm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}